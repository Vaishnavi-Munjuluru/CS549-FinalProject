{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ee09abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8da35c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sno</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>fyear</th>\n",
       "      <th>target</th>\n",
       "      <th>at</th>\n",
       "      <th>revt</th>\n",
       "      <th>ib</th>\n",
       "      <th>ca</th>\n",
       "      <th>capx</th>\n",
       "      <th>ceq</th>\n",
       "      <th>che</th>\n",
       "      <th>emp</th>\n",
       "      <th>invt</th>\n",
       "      <th>re</th>\n",
       "      <th>xrd</th>\n",
       "      <th>dltt</th>\n",
       "      <th>dv</th>\n",
       "      <th>sic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1004</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>701.854</td>\n",
       "      <td>874.255</td>\n",
       "      <td>18.531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.134</td>\n",
       "      <td>340.212</td>\n",
       "      <td>13.809</td>\n",
       "      <td>2.5</td>\n",
       "      <td>320.590</td>\n",
       "      <td>204.065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.987</td>\n",
       "      <td>9.157</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>1004</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>710.199</td>\n",
       "      <td>638.721</td>\n",
       "      <td>-58.939</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.112</td>\n",
       "      <td>310.235</td>\n",
       "      <td>34.522</td>\n",
       "      <td>2.2</td>\n",
       "      <td>286.588</td>\n",
       "      <td>139.603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>217.699</td>\n",
       "      <td>4.430</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>1004</td>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>686.621</td>\n",
       "      <td>606.337</td>\n",
       "      <td>-12.410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.930</td>\n",
       "      <td>294.988</td>\n",
       "      <td>29.154</td>\n",
       "      <td>2.1</td>\n",
       "      <td>259.954</td>\n",
       "      <td>124.106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164.658</td>\n",
       "      <td>0.797</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1004</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>709.292</td>\n",
       "      <td>651.958</td>\n",
       "      <td>3.504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.286</td>\n",
       "      <td>301.684</td>\n",
       "      <td>41.010</td>\n",
       "      <td>2.3</td>\n",
       "      <td>247.245</td>\n",
       "      <td>131.884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>248.666</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1004</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>732.230</td>\n",
       "      <td>747.848</td>\n",
       "      <td>18.572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.033</td>\n",
       "      <td>314.744</td>\n",
       "      <td>50.338</td>\n",
       "      <td>2.6</td>\n",
       "      <td>255.477</td>\n",
       "      <td>142.450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>227.159</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225005</th>\n",
       "      <td>294674</td>\n",
       "      <td>335466</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225006</th>\n",
       "      <td>294675</td>\n",
       "      <td>335466</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225007</th>\n",
       "      <td>294676</td>\n",
       "      <td>345980</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5961.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225008</th>\n",
       "      <td>294677</td>\n",
       "      <td>345980</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5961.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225009</th>\n",
       "      <td>294678</td>\n",
       "      <td>345980</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5961.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225010 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sno   gvkey  fyear  target       at     revt      ib  ca    capx  \\\n",
       "0            6    1004   2000       0  701.854  874.255  18.531 NaN  13.134   \n",
       "1            7    1004   2001       0  710.199  638.721 -58.939 NaN  12.112   \n",
       "2            8    1004   2002       0  686.621  606.337 -12.410 NaN   9.930   \n",
       "3            9    1004   2003       0  709.292  651.958   3.504 NaN  10.286   \n",
       "4           10    1004   2004       0  732.230  747.848  18.572 NaN  13.033   \n",
       "...        ...     ...    ...     ...      ...      ...     ...  ..     ...   \n",
       "225005  294674  335466   2016       0      NaN      NaN     NaN NaN     NaN   \n",
       "225006  294675  335466   2017       0      NaN      NaN     NaN NaN     NaN   \n",
       "225007  294676  345980   2015       0      NaN      NaN     NaN NaN     NaN   \n",
       "225008  294677  345980   2016       0      NaN      NaN     NaN NaN     NaN   \n",
       "225009  294678  345980   2017       0      NaN      NaN     NaN NaN     NaN   \n",
       "\n",
       "            ceq     che  emp     invt       re  xrd     dltt     dv     sic  \n",
       "0       340.212  13.809  2.5  320.590  204.065  NaN  179.987  9.157  5080.0  \n",
       "1       310.235  34.522  2.2  286.588  139.603  NaN  217.699  4.430  5080.0  \n",
       "2       294.988  29.154  2.1  259.954  124.106  NaN  164.658  0.797  5080.0  \n",
       "3       301.684  41.010  2.3  247.245  131.884  NaN  248.666  0.000  5080.0  \n",
       "4       314.744  50.338  2.6  255.477  142.450  NaN  227.159  0.000  5080.0  \n",
       "...         ...     ...  ...      ...      ...  ...      ...    ...     ...  \n",
       "225005      NaN     NaN  NaN      NaN      NaN  NaN      NaN    NaN     NaN  \n",
       "225006      NaN     NaN  NaN      NaN      NaN  NaN      NaN    NaN     NaN  \n",
       "225007      NaN     NaN  NaN      NaN      NaN  NaN      NaN    NaN  5961.0  \n",
       "225008      NaN     NaN  NaN      NaN      NaN  NaN      NaN    NaN  5961.0  \n",
       "225009      NaN     NaN  NaN      NaN      NaN  NaN      NaN    NaN  5961.0  \n",
       "\n",
       "[225010 rows x 18 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to read the data set\n",
    "data= pd.read_csv(\"targetfirm_prediction_dataset_small.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9169fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sno', 'gvkey', 'fyear', 'target', 'at', 'revt', 'ib', 'ca', 'capx',\n",
       "       'ceq', 'che', 'emp', 'invt', 're', 'xrd', 'dltt', 'dv', 'sic'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed3150c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sno            0\n",
       "gvkey          0\n",
       "fyear          0\n",
       "target         0\n",
       "at         34635\n",
       "revt       35677\n",
       "ib         35660\n",
       "ca        211399\n",
       "capx       63184\n",
       "ceq        35063\n",
       "che        57076\n",
       "emp        63859\n",
       "invt       53353\n",
       "re         40548\n",
       "xrd       150287\n",
       "dltt       35036\n",
       "dv         65058\n",
       "sic            3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the number of missing coloumns\n",
    "num_missing = (data[data.columns].isnull()).sum()\n",
    "num_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebd886d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sno            0\n",
       "target    223149\n",
       "at           814\n",
       "revt       16382\n",
       "ib           200\n",
       "capx       17174\n",
       "ceq          194\n",
       "che         4907\n",
       "emp         5259\n",
       "invt       65845\n",
       "re           429\n",
       "dltt       52103\n",
       "dv        101392\n",
       "sic            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finds the number of Nan values for each column in the dataset and dropping the coloumns ca, xrd \n",
    "data2 = data.drop(['ca','xrd','gvkey','fyear'],1)\n",
    "num_missing = (data2[data2.columns].isnull()).sum()\n",
    "num_missing1 = (data2[data2.columns]==0.0).sum()\n",
    "num_missing1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a88434a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sno</th>\n",
       "      <th>target</th>\n",
       "      <th>at</th>\n",
       "      <th>revt</th>\n",
       "      <th>ib</th>\n",
       "      <th>capx</th>\n",
       "      <th>ceq</th>\n",
       "      <th>che</th>\n",
       "      <th>emp</th>\n",
       "      <th>invt</th>\n",
       "      <th>re</th>\n",
       "      <th>dltt</th>\n",
       "      <th>dv</th>\n",
       "      <th>sic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>701.854</td>\n",
       "      <td>874.255</td>\n",
       "      <td>18.531</td>\n",
       "      <td>13.134</td>\n",
       "      <td>340.212</td>\n",
       "      <td>13.809</td>\n",
       "      <td>2.500</td>\n",
       "      <td>320.590</td>\n",
       "      <td>204.065</td>\n",
       "      <td>179.987</td>\n",
       "      <td>9.157</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>710.199</td>\n",
       "      <td>638.721</td>\n",
       "      <td>-58.939</td>\n",
       "      <td>12.112</td>\n",
       "      <td>310.235</td>\n",
       "      <td>34.522</td>\n",
       "      <td>2.200</td>\n",
       "      <td>286.588</td>\n",
       "      <td>139.603</td>\n",
       "      <td>217.699</td>\n",
       "      <td>4.430</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>686.621</td>\n",
       "      <td>606.337</td>\n",
       "      <td>-12.410</td>\n",
       "      <td>9.930</td>\n",
       "      <td>294.988</td>\n",
       "      <td>29.154</td>\n",
       "      <td>2.100</td>\n",
       "      <td>259.954</td>\n",
       "      <td>124.106</td>\n",
       "      <td>164.658</td>\n",
       "      <td>0.797</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>709.292</td>\n",
       "      <td>651.958</td>\n",
       "      <td>3.504</td>\n",
       "      <td>10.286</td>\n",
       "      <td>301.684</td>\n",
       "      <td>41.010</td>\n",
       "      <td>2.300</td>\n",
       "      <td>247.245</td>\n",
       "      <td>131.884</td>\n",
       "      <td>248.666</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>732.230</td>\n",
       "      <td>747.848</td>\n",
       "      <td>18.572</td>\n",
       "      <td>13.033</td>\n",
       "      <td>314.744</td>\n",
       "      <td>50.338</td>\n",
       "      <td>2.600</td>\n",
       "      <td>255.477</td>\n",
       "      <td>142.450</td>\n",
       "      <td>227.159</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224977</th>\n",
       "      <td>294646</td>\n",
       "      <td>0</td>\n",
       "      <td>28.426</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-40.324</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.809</td>\n",
       "      <td>21.830</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-50.268</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224978</th>\n",
       "      <td>294647</td>\n",
       "      <td>0</td>\n",
       "      <td>29.188</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-44.810</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-8.908</td>\n",
       "      <td>22.651</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-93.370</td>\n",
       "      <td>14.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224984</th>\n",
       "      <td>294653</td>\n",
       "      <td>0</td>\n",
       "      <td>51.334</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-39.892</td>\n",
       "      <td>0.291</td>\n",
       "      <td>35.513</td>\n",
       "      <td>50.573</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-90.284</td>\n",
       "      <td>7.412</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224985</th>\n",
       "      <td>294654</td>\n",
       "      <td>0</td>\n",
       "      <td>361.871</td>\n",
       "      <td>198.075</td>\n",
       "      <td>13.905</td>\n",
       "      <td>31.421</td>\n",
       "      <td>280.301</td>\n",
       "      <td>36.835</td>\n",
       "      <td>0.097</td>\n",
       "      <td>8.215</td>\n",
       "      <td>241.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.733</td>\n",
       "      <td>1311.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224996</th>\n",
       "      <td>294665</td>\n",
       "      <td>0</td>\n",
       "      <td>507.122</td>\n",
       "      <td>409.522</td>\n",
       "      <td>-60.812</td>\n",
       "      <td>1.212</td>\n",
       "      <td>319.736</td>\n",
       "      <td>46.522</td>\n",
       "      <td>0.808</td>\n",
       "      <td>9.078</td>\n",
       "      <td>-153.433</td>\n",
       "      <td>20.790</td>\n",
       "      <td>1.674</td>\n",
       "      <td>4412.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129486 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sno  target       at     revt      ib    capx      ceq     che  \\\n",
       "0            6       0  701.854  874.255  18.531  13.134  340.212  13.809   \n",
       "1            7       0  710.199  638.721 -58.939  12.112  310.235  34.522   \n",
       "2            8       0  686.621  606.337 -12.410   9.930  294.988  29.154   \n",
       "3            9       0  709.292  651.958   3.504  10.286  301.684  41.010   \n",
       "4           10       0  732.230  747.848  18.572  13.033  314.744  50.338   \n",
       "...        ...     ...      ...      ...     ...     ...      ...     ...   \n",
       "224977  294646       0   28.426    0.000 -40.324   0.000    9.809  21.830   \n",
       "224978  294647       0   29.188    0.000 -44.810   0.000   -8.908  22.651   \n",
       "224984  294653       0   51.334    0.000 -39.892   0.291   35.513  50.573   \n",
       "224985  294654       0  361.871  198.075  13.905  31.421  280.301  36.835   \n",
       "224996  294665       0  507.122  409.522 -60.812   1.212  319.736  46.522   \n",
       "\n",
       "          emp     invt       re     dltt     dv     sic  \n",
       "0       2.500  320.590  204.065  179.987  9.157  5080.0  \n",
       "1       2.200  286.588  139.603  217.699  4.430  5080.0  \n",
       "2       2.100  259.954  124.106  164.658  0.797  5080.0  \n",
       "3       2.300  247.245  131.884  248.666  0.000  5080.0  \n",
       "4       2.600  255.477  142.450  227.159  0.000  5080.0  \n",
       "...       ...      ...      ...      ...    ...     ...  \n",
       "224977  0.006    0.000  -50.268    0.000  0.000  2836.0  \n",
       "224978  0.008    0.000  -93.370   14.057  0.000  2836.0  \n",
       "224984  0.047    0.000  -90.284    7.412  0.000  2836.0  \n",
       "224985  0.097    8.215  241.062    0.000  6.733  1311.0  \n",
       "224996  0.808    9.078 -153.433   20.790  1.674  4412.0  \n",
       "\n",
       "[129486 rows x 14 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning the dataset by removing the NAN values\n",
    "data2 = data2.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c93fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from numpy import log,dot,e,shape\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dc4f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data2.drop(['target'],1)\n",
    "y= data2.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6f0fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65c18105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "#\n",
    "y_pred=logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2340cfb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25639,     1],\n",
       "       [  258,     0]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e93d3e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9899992277395938\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56e2d31e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-992aad97278e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     ])\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0msvm_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n\u001b[0m\u001b[0;32m    235\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_scaling\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[0msolver_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_liblinear_solver_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 975\u001b[1;33m     raw_coef_, n_iter_ = liblinear.train_wrap(\n\u001b[0m\u001b[0;32m    976\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Linear SVM\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X = data2.drop(['target'],1)\n",
    "y= data2.target\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\",max_iter=10000 )),\n",
    "    ])\n",
    "\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8762ceb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svm_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d3f9482a84e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#metrics calculations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcnf_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'svm_clf' is not defined"
     ]
    }
   ],
   "source": [
    "#metrics calculations\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27aad9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sno</th>\n",
       "      <th>target</th>\n",
       "      <th>at</th>\n",
       "      <th>revt</th>\n",
       "      <th>ib</th>\n",
       "      <th>capx</th>\n",
       "      <th>ceq</th>\n",
       "      <th>che</th>\n",
       "      <th>emp</th>\n",
       "      <th>invt</th>\n",
       "      <th>re</th>\n",
       "      <th>dltt</th>\n",
       "      <th>dv</th>\n",
       "      <th>sic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>701.854</td>\n",
       "      <td>874.255</td>\n",
       "      <td>18.531</td>\n",
       "      <td>13.134</td>\n",
       "      <td>340.212</td>\n",
       "      <td>13.809</td>\n",
       "      <td>2.500</td>\n",
       "      <td>320.590</td>\n",
       "      <td>204.065</td>\n",
       "      <td>179.987</td>\n",
       "      <td>9.157</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>710.199</td>\n",
       "      <td>638.721</td>\n",
       "      <td>-58.939</td>\n",
       "      <td>12.112</td>\n",
       "      <td>310.235</td>\n",
       "      <td>34.522</td>\n",
       "      <td>2.200</td>\n",
       "      <td>286.588</td>\n",
       "      <td>139.603</td>\n",
       "      <td>217.699</td>\n",
       "      <td>4.430</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>686.621</td>\n",
       "      <td>606.337</td>\n",
       "      <td>-12.410</td>\n",
       "      <td>9.930</td>\n",
       "      <td>294.988</td>\n",
       "      <td>29.154</td>\n",
       "      <td>2.100</td>\n",
       "      <td>259.954</td>\n",
       "      <td>124.106</td>\n",
       "      <td>164.658</td>\n",
       "      <td>0.797</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>709.292</td>\n",
       "      <td>651.958</td>\n",
       "      <td>3.504</td>\n",
       "      <td>10.286</td>\n",
       "      <td>301.684</td>\n",
       "      <td>41.010</td>\n",
       "      <td>2.300</td>\n",
       "      <td>247.245</td>\n",
       "      <td>131.884</td>\n",
       "      <td>248.666</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>732.230</td>\n",
       "      <td>747.848</td>\n",
       "      <td>18.572</td>\n",
       "      <td>13.033</td>\n",
       "      <td>314.744</td>\n",
       "      <td>50.338</td>\n",
       "      <td>2.600</td>\n",
       "      <td>255.477</td>\n",
       "      <td>142.450</td>\n",
       "      <td>227.159</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224977</th>\n",
       "      <td>294646</td>\n",
       "      <td>0</td>\n",
       "      <td>28.426</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-40.324</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.809</td>\n",
       "      <td>21.830</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-50.268</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224978</th>\n",
       "      <td>294647</td>\n",
       "      <td>0</td>\n",
       "      <td>29.188</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-44.810</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-8.908</td>\n",
       "      <td>22.651</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-93.370</td>\n",
       "      <td>14.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224984</th>\n",
       "      <td>294653</td>\n",
       "      <td>0</td>\n",
       "      <td>51.334</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-39.892</td>\n",
       "      <td>0.291</td>\n",
       "      <td>35.513</td>\n",
       "      <td>50.573</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-90.284</td>\n",
       "      <td>7.412</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224985</th>\n",
       "      <td>294654</td>\n",
       "      <td>0</td>\n",
       "      <td>361.871</td>\n",
       "      <td>198.075</td>\n",
       "      <td>13.905</td>\n",
       "      <td>31.421</td>\n",
       "      <td>280.301</td>\n",
       "      <td>36.835</td>\n",
       "      <td>0.097</td>\n",
       "      <td>8.215</td>\n",
       "      <td>241.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.733</td>\n",
       "      <td>1311.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224996</th>\n",
       "      <td>294665</td>\n",
       "      <td>0</td>\n",
       "      <td>507.122</td>\n",
       "      <td>409.522</td>\n",
       "      <td>-60.812</td>\n",
       "      <td>1.212</td>\n",
       "      <td>319.736</td>\n",
       "      <td>46.522</td>\n",
       "      <td>0.808</td>\n",
       "      <td>9.078</td>\n",
       "      <td>-153.433</td>\n",
       "      <td>20.790</td>\n",
       "      <td>1.674</td>\n",
       "      <td>4412.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129486 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sno  target       at     revt      ib    capx      ceq     che  \\\n",
       "0            6       0  701.854  874.255  18.531  13.134  340.212  13.809   \n",
       "1            7       0  710.199  638.721 -58.939  12.112  310.235  34.522   \n",
       "2            8       0  686.621  606.337 -12.410   9.930  294.988  29.154   \n",
       "3            9       0  709.292  651.958   3.504  10.286  301.684  41.010   \n",
       "4           10       0  732.230  747.848  18.572  13.033  314.744  50.338   \n",
       "...        ...     ...      ...      ...     ...     ...      ...     ...   \n",
       "224977  294646       0   28.426    0.000 -40.324   0.000    9.809  21.830   \n",
       "224978  294647       0   29.188    0.000 -44.810   0.000   -8.908  22.651   \n",
       "224984  294653       0   51.334    0.000 -39.892   0.291   35.513  50.573   \n",
       "224985  294654       0  361.871  198.075  13.905  31.421  280.301  36.835   \n",
       "224996  294665       0  507.122  409.522 -60.812   1.212  319.736  46.522   \n",
       "\n",
       "          emp     invt       re     dltt     dv     sic  \n",
       "0       2.500  320.590  204.065  179.987  9.157  5080.0  \n",
       "1       2.200  286.588  139.603  217.699  4.430  5080.0  \n",
       "2       2.100  259.954  124.106  164.658  0.797  5080.0  \n",
       "3       2.300  247.245  131.884  248.666  0.000  5080.0  \n",
       "4       2.600  255.477  142.450  227.159  0.000  5080.0  \n",
       "...       ...      ...      ...      ...    ...     ...  \n",
       "224977  0.006    0.000  -50.268    0.000  0.000  2836.0  \n",
       "224978  0.008    0.000  -93.370   14.057  0.000  2836.0  \n",
       "224984  0.047    0.000  -90.284    7.412  0.000  2836.0  \n",
       "224985  0.097    8.215  241.062    0.000  6.733  1311.0  \n",
       "224996  0.808    9.078 -153.433   20.790  1.674  4412.0  \n",
       "\n",
       "[129486 rows x 14 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48a74695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow/Keras: 2.8.0\n",
      "pandas: 1.2.4\n",
      "numpy: 1.20.1\n",
      "sklearn: 0.24.1\n",
      "plotly: 5.7.0\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow / Keras\n",
    "from tensorflow import keras # for building Neural Networks\n",
    "print('Tensorflow/Keras: %s' % keras.__version__) # print version\n",
    "from keras.models import Sequential # for creating a linear stack of layers for our Neural Network\n",
    "from keras import Input # for instantiating a keras tensor\n",
    "from keras.layers import Dense # for creating regular densely-connected NN layers.\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd # for data manipulation\n",
    "print('pandas: %s' % pd.__version__) # print version\n",
    "import numpy as np # for data manipulation\n",
    "print('numpy: %s' % np.__version__) # print version\n",
    "\n",
    "# Sklearn\n",
    "import sklearn # for model evaluation\n",
    "print('sklearn: %s' % sklearn.__version__) # print version\n",
    "from sklearn.model_selection import train_test_split # for splitting data into train and test samples\n",
    "from sklearn.metrics import classification_report # for model evaluation metrics\n",
    "\n",
    "# Visualization\n",
    "import plotly \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "print('plotly: %s' % plotly.__version__) # print version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6139e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data2.drop(['target'],1)\n",
    "y= data2.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5a78c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sno</th>\n",
       "      <th>at</th>\n",
       "      <th>revt</th>\n",
       "      <th>ib</th>\n",
       "      <th>capx</th>\n",
       "      <th>ceq</th>\n",
       "      <th>che</th>\n",
       "      <th>emp</th>\n",
       "      <th>invt</th>\n",
       "      <th>re</th>\n",
       "      <th>dltt</th>\n",
       "      <th>dv</th>\n",
       "      <th>sic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>701.854</td>\n",
       "      <td>874.255</td>\n",
       "      <td>18.531</td>\n",
       "      <td>13.134</td>\n",
       "      <td>340.212</td>\n",
       "      <td>13.809</td>\n",
       "      <td>2.500</td>\n",
       "      <td>320.590</td>\n",
       "      <td>204.065</td>\n",
       "      <td>179.987</td>\n",
       "      <td>9.157</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>710.199</td>\n",
       "      <td>638.721</td>\n",
       "      <td>-58.939</td>\n",
       "      <td>12.112</td>\n",
       "      <td>310.235</td>\n",
       "      <td>34.522</td>\n",
       "      <td>2.200</td>\n",
       "      <td>286.588</td>\n",
       "      <td>139.603</td>\n",
       "      <td>217.699</td>\n",
       "      <td>4.430</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>686.621</td>\n",
       "      <td>606.337</td>\n",
       "      <td>-12.410</td>\n",
       "      <td>9.930</td>\n",
       "      <td>294.988</td>\n",
       "      <td>29.154</td>\n",
       "      <td>2.100</td>\n",
       "      <td>259.954</td>\n",
       "      <td>124.106</td>\n",
       "      <td>164.658</td>\n",
       "      <td>0.797</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>709.292</td>\n",
       "      <td>651.958</td>\n",
       "      <td>3.504</td>\n",
       "      <td>10.286</td>\n",
       "      <td>301.684</td>\n",
       "      <td>41.010</td>\n",
       "      <td>2.300</td>\n",
       "      <td>247.245</td>\n",
       "      <td>131.884</td>\n",
       "      <td>248.666</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>732.230</td>\n",
       "      <td>747.848</td>\n",
       "      <td>18.572</td>\n",
       "      <td>13.033</td>\n",
       "      <td>314.744</td>\n",
       "      <td>50.338</td>\n",
       "      <td>2.600</td>\n",
       "      <td>255.477</td>\n",
       "      <td>142.450</td>\n",
       "      <td>227.159</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224977</th>\n",
       "      <td>294646</td>\n",
       "      <td>28.426</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-40.324</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.809</td>\n",
       "      <td>21.830</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-50.268</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224978</th>\n",
       "      <td>294647</td>\n",
       "      <td>29.188</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-44.810</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-8.908</td>\n",
       "      <td>22.651</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-93.370</td>\n",
       "      <td>14.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224984</th>\n",
       "      <td>294653</td>\n",
       "      <td>51.334</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-39.892</td>\n",
       "      <td>0.291</td>\n",
       "      <td>35.513</td>\n",
       "      <td>50.573</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-90.284</td>\n",
       "      <td>7.412</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224985</th>\n",
       "      <td>294654</td>\n",
       "      <td>361.871</td>\n",
       "      <td>198.075</td>\n",
       "      <td>13.905</td>\n",
       "      <td>31.421</td>\n",
       "      <td>280.301</td>\n",
       "      <td>36.835</td>\n",
       "      <td>0.097</td>\n",
       "      <td>8.215</td>\n",
       "      <td>241.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.733</td>\n",
       "      <td>1311.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224996</th>\n",
       "      <td>294665</td>\n",
       "      <td>507.122</td>\n",
       "      <td>409.522</td>\n",
       "      <td>-60.812</td>\n",
       "      <td>1.212</td>\n",
       "      <td>319.736</td>\n",
       "      <td>46.522</td>\n",
       "      <td>0.808</td>\n",
       "      <td>9.078</td>\n",
       "      <td>-153.433</td>\n",
       "      <td>20.790</td>\n",
       "      <td>1.674</td>\n",
       "      <td>4412.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129486 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sno       at     revt      ib    capx      ceq     che    emp  \\\n",
       "0            6  701.854  874.255  18.531  13.134  340.212  13.809  2.500   \n",
       "1            7  710.199  638.721 -58.939  12.112  310.235  34.522  2.200   \n",
       "2            8  686.621  606.337 -12.410   9.930  294.988  29.154  2.100   \n",
       "3            9  709.292  651.958   3.504  10.286  301.684  41.010  2.300   \n",
       "4           10  732.230  747.848  18.572  13.033  314.744  50.338  2.600   \n",
       "...        ...      ...      ...     ...     ...      ...     ...    ...   \n",
       "224977  294646   28.426    0.000 -40.324   0.000    9.809  21.830  0.006   \n",
       "224978  294647   29.188    0.000 -44.810   0.000   -8.908  22.651  0.008   \n",
       "224984  294653   51.334    0.000 -39.892   0.291   35.513  50.573  0.047   \n",
       "224985  294654  361.871  198.075  13.905  31.421  280.301  36.835  0.097   \n",
       "224996  294665  507.122  409.522 -60.812   1.212  319.736  46.522  0.808   \n",
       "\n",
       "           invt       re     dltt     dv     sic  \n",
       "0       320.590  204.065  179.987  9.157  5080.0  \n",
       "1       286.588  139.603  217.699  4.430  5080.0  \n",
       "2       259.954  124.106  164.658  0.797  5080.0  \n",
       "3       247.245  131.884  248.666  0.000  5080.0  \n",
       "4       255.477  142.450  227.159  0.000  5080.0  \n",
       "...         ...      ...      ...    ...     ...  \n",
       "224977    0.000  -50.268    0.000  0.000  2836.0  \n",
       "224978    0.000  -93.370   14.057  0.000  2836.0  \n",
       "224984    0.000  -90.284    7.412  0.000  2836.0  \n",
       "224985    8.215  241.062    0.000  6.733  1311.0  \n",
       "224996    9.078 -153.433   20.790  1.674  4412.0  \n",
       "\n",
       "[129486 rows x 13 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa74b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 3 - Specify the structure of a Neural Network\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=13, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7bb138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d3a18f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "12949/12949 [==============================] - 15s 1ms/step - loss: 61.3813 - accuracy: 0.9802\n",
      "Epoch 2/7\n",
      "12949/12949 [==============================] - 14s 1ms/step - loss: 35.5982 - accuracy: 0.9807\n",
      "Epoch 3/7\n",
      "12949/12949 [==============================] - 14s 1ms/step - loss: 14.9013 - accuracy: 0.9807\n",
      "Epoch 4/7\n",
      "12949/12949 [==============================] - 15s 1ms/step - loss: 4.8273 - accuracy: 0.9822\n",
      "Epoch 5/7\n",
      "12949/12949 [==============================] - 15s 1ms/step - loss: 0.6289 - accuracy: 0.9902\n",
      "Epoch 6/7\n",
      "12949/12949 [==============================] - 14s 1ms/step - loss: 0.1116 - accuracy: 0.9906\n",
      "Epoch 7/7\n",
      "12949/12949 [==============================] - 15s 1ms/step - loss: 0.0910 - accuracy: 0.9906\n"
     ]
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "history = model.fit(X, y, epochs=7, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3597aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_acc = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "694600ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.3.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.20.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5106cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37f3a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM ragged tensor\n",
    "data3 = data.drop(['sno','ca','xrd'],1)\n",
    "data3 = data3.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "\n",
    "year_data = data3.drop(data3.iloc[:, 3:18], inplace = False, axis = 1)\n",
    "\n",
    "#data3 = data3.drop(['gvkey','fyear'],1)\n",
    "#data3 = data3.drop(['gvkey','fyear'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24e3bdda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gvkey</th>\n",
       "      <th>fyear</th>\n",
       "      <th>target</th>\n",
       "      <th>at</th>\n",
       "      <th>revt</th>\n",
       "      <th>ib</th>\n",
       "      <th>capx</th>\n",
       "      <th>ceq</th>\n",
       "      <th>che</th>\n",
       "      <th>emp</th>\n",
       "      <th>invt</th>\n",
       "      <th>re</th>\n",
       "      <th>dltt</th>\n",
       "      <th>dv</th>\n",
       "      <th>sic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1004</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>701.854</td>\n",
       "      <td>874.255</td>\n",
       "      <td>18.531</td>\n",
       "      <td>13.134</td>\n",
       "      <td>340.212</td>\n",
       "      <td>13.809</td>\n",
       "      <td>2.500</td>\n",
       "      <td>320.590</td>\n",
       "      <td>204.065</td>\n",
       "      <td>179.987</td>\n",
       "      <td>9.157</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1004</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>710.199</td>\n",
       "      <td>638.721</td>\n",
       "      <td>-58.939</td>\n",
       "      <td>12.112</td>\n",
       "      <td>310.235</td>\n",
       "      <td>34.522</td>\n",
       "      <td>2.200</td>\n",
       "      <td>286.588</td>\n",
       "      <td>139.603</td>\n",
       "      <td>217.699</td>\n",
       "      <td>4.430</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1004</td>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>686.621</td>\n",
       "      <td>606.337</td>\n",
       "      <td>-12.410</td>\n",
       "      <td>9.930</td>\n",
       "      <td>294.988</td>\n",
       "      <td>29.154</td>\n",
       "      <td>2.100</td>\n",
       "      <td>259.954</td>\n",
       "      <td>124.106</td>\n",
       "      <td>164.658</td>\n",
       "      <td>0.797</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>709.292</td>\n",
       "      <td>651.958</td>\n",
       "      <td>3.504</td>\n",
       "      <td>10.286</td>\n",
       "      <td>301.684</td>\n",
       "      <td>41.010</td>\n",
       "      <td>2.300</td>\n",
       "      <td>247.245</td>\n",
       "      <td>131.884</td>\n",
       "      <td>248.666</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>732.230</td>\n",
       "      <td>747.848</td>\n",
       "      <td>18.572</td>\n",
       "      <td>13.033</td>\n",
       "      <td>314.744</td>\n",
       "      <td>50.338</td>\n",
       "      <td>2.600</td>\n",
       "      <td>255.477</td>\n",
       "      <td>142.450</td>\n",
       "      <td>227.159</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224977</th>\n",
       "      <td>319507</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>28.426</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-40.324</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.809</td>\n",
       "      <td>21.830</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-50.268</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224978</th>\n",
       "      <td>319507</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>29.188</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-44.810</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-8.908</td>\n",
       "      <td>22.651</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-93.370</td>\n",
       "      <td>14.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224984</th>\n",
       "      <td>324684</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>51.334</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-39.892</td>\n",
       "      <td>0.291</td>\n",
       "      <td>35.513</td>\n",
       "      <td>50.573</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-90.284</td>\n",
       "      <td>7.412</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224985</th>\n",
       "      <td>325211</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>361.871</td>\n",
       "      <td>198.075</td>\n",
       "      <td>13.905</td>\n",
       "      <td>31.421</td>\n",
       "      <td>280.301</td>\n",
       "      <td>36.835</td>\n",
       "      <td>0.097</td>\n",
       "      <td>8.215</td>\n",
       "      <td>241.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.733</td>\n",
       "      <td>1311.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224996</th>\n",
       "      <td>327451</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>507.122</td>\n",
       "      <td>409.522</td>\n",
       "      <td>-60.812</td>\n",
       "      <td>1.212</td>\n",
       "      <td>319.736</td>\n",
       "      <td>46.522</td>\n",
       "      <td>0.808</td>\n",
       "      <td>9.078</td>\n",
       "      <td>-153.433</td>\n",
       "      <td>20.790</td>\n",
       "      <td>1.674</td>\n",
       "      <td>4412.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129486 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         gvkey  fyear  target       at     revt      ib    capx      ceq  \\\n",
       "0         1004   2000       0  701.854  874.255  18.531  13.134  340.212   \n",
       "1         1004   2001       0  710.199  638.721 -58.939  12.112  310.235   \n",
       "2         1004   2002       0  686.621  606.337 -12.410   9.930  294.988   \n",
       "3         1004   2003       0  709.292  651.958   3.504  10.286  301.684   \n",
       "4         1004   2004       0  732.230  747.848  18.572  13.033  314.744   \n",
       "...        ...    ...     ...      ...      ...     ...     ...      ...   \n",
       "224977  319507   2016       0   28.426    0.000 -40.324   0.000    9.809   \n",
       "224978  319507   2017       0   29.188    0.000 -44.810   0.000   -8.908   \n",
       "224984  324684   2017       0   51.334    0.000 -39.892   0.291   35.513   \n",
       "224985  325211   2017       0  361.871  198.075  13.905  31.421  280.301   \n",
       "224996  327451   2017       0  507.122  409.522 -60.812   1.212  319.736   \n",
       "\n",
       "           che    emp     invt       re     dltt     dv     sic  \n",
       "0       13.809  2.500  320.590  204.065  179.987  9.157  5080.0  \n",
       "1       34.522  2.200  286.588  139.603  217.699  4.430  5080.0  \n",
       "2       29.154  2.100  259.954  124.106  164.658  0.797  5080.0  \n",
       "3       41.010  2.300  247.245  131.884  248.666  0.000  5080.0  \n",
       "4       50.338  2.600  255.477  142.450  227.159  0.000  5080.0  \n",
       "...        ...    ...      ...      ...      ...    ...     ...  \n",
       "224977  21.830  0.006    0.000  -50.268    0.000  0.000  2836.0  \n",
       "224978  22.651  0.008    0.000  -93.370   14.057  0.000  2836.0  \n",
       "224984  50.573  0.047    0.000  -90.284    7.412  0.000  2836.0  \n",
       "224985  36.835  0.097    8.215  241.062    0.000  6.733  1311.0  \n",
       "224996  46.522  0.808    9.078 -153.433   20.790  1.674  4412.0  \n",
       "\n",
       "[129486 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d45cc9d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gvkey</th>\n",
       "      <th>fyear</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1004</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1004</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1004</td>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224977</th>\n",
       "      <td>319507</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224978</th>\n",
       "      <td>319507</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224984</th>\n",
       "      <td>324684</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224985</th>\n",
       "      <td>325211</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224996</th>\n",
       "      <td>327451</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129486 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         gvkey  fyear  target\n",
       "0         1004   2000       0\n",
       "1         1004   2001       0\n",
       "2         1004   2002       0\n",
       "3         1004   2003       0\n",
       "4         1004   2004       0\n",
       "...        ...    ...     ...\n",
       "224977  319507   2016       0\n",
       "224978  319507   2017       0\n",
       "224984  324684   2017       0\n",
       "224985  325211   2017       0\n",
       "224996  327451   2017       0\n",
       "\n",
       "[129486 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f383cf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gvkey</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224977</th>\n",
       "      <td>319507</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224978</th>\n",
       "      <td>319507</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224984</th>\n",
       "      <td>324684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224985</th>\n",
       "      <td>325211</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224996</th>\n",
       "      <td>327451</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129486 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         gvkey  target\n",
       "0         1004       0\n",
       "1         1004       0\n",
       "2         1004       0\n",
       "3         1004       0\n",
       "4         1004       0\n",
       "...        ...     ...\n",
       "224977  319507       0\n",
       "224978  319507       0\n",
       "224984  324684       0\n",
       "224985  325211       0\n",
       "224996  327451       0\n",
       "\n",
       "[129486 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_old = data3.drop(['target'],1)\n",
    "Y_old = data3.target\n",
    "\n",
    "\n",
    "#X_old = X_old.to_numpy(dtype = 'float32')\n",
    "#Y_old = data3.target(dtype = 'int32')\n",
    "\n",
    "Y_oldd = year_data.drop(['fyear'],1)\n",
    "Y_oldd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31b5d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifying the dataframe for ragged data\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "\n",
    "lookback =10\n",
    "start = 0\n",
    "index = 0\n",
    "end = 0\n",
    "maxLen = end - start \n",
    "while index< len(year_data)-1: \n",
    "    cid = year_data.iloc[index]['gvkey']\n",
    "    start = index\n",
    "    end = index\n",
    "    while end<len(year_data):\n",
    "        if year_data.iloc[end]['gvkey'] == year_data.iloc[index]['gvkey']:\n",
    "            end+=1\n",
    "            \n",
    "        else:\n",
    "            #sample.append(data3[start: end])\n",
    "            X.append(X_old[start: end])\n",
    "            Y.append(Y_old[start: end])\n",
    "            index = end\n",
    "            break\n",
    "            \n",
    "    if maxLen < (end - start):\n",
    "        maxLen = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36291709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def get_ragged_constants(data):\n",
    "    return tf.RaggedTensor.from_row_lengths(\n",
    "        values=data.values,\n",
    "        row_lengths=data.groupby('gvkey').size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf8b43f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[[1004.0, 2000.0, 701.854, 874.255, 18.531, 13.134, 340.212, 13.809, 2.5,\n",
       "   320.59, 204.065, 179.987, 9.157, 5080.0],\n",
       "  [1004.0, 2001.0, 710.199, 638.721, -58.939, 12.112, 310.235, 34.522, 2.2,\n",
       "   286.588, 139.603, 217.699, 4.43, 5080.0],\n",
       "  [1004.0, 2002.0, 686.621, 606.337, -12.41, 9.93, 294.988, 29.154, 2.1,\n",
       "   259.954, 124.106, 164.658, 0.797, 5080.0],\n",
       "  [1004.0, 2003.0, 709.292, 651.958, 3.504, 10.286, 301.684, 41.01, 2.3,\n",
       "   247.245, 131.884, 248.666, 0.0, 5080.0],\n",
       "  [1004.0, 2004.0, 732.23, 747.848, 18.572, 13.033, 314.744, 50.338, 2.6,\n",
       "   255.477, 142.45, 227.159, 0.0, 5080.0],\n",
       "  [1004.0, 2005.0, 978.819, 897.284, 35.163, 16.296, 422.717, 121.738, 3.3,\n",
       "   323.592, 183.55, 318.576, 0.0, 5080.0],\n",
       "  [1004.0, 2006.0, 1067.633, 1061.169, 59.447, 29.891, 494.243, 83.317,\n",
       "   3.9, 342.593, 242.153, 253.611, 0.0, 5080.0],\n",
       "  [1004.0, 2007.0, 1362.01, 1384.919, 75.745, 30.334, 585.255, 112.435,\n",
       "   5.4, 435.608, 318.184, 507.918, 0.0, 5080.0],\n",
       "  [1004.0, 2008.0, 1377.511, 1423.976, 80.6, 27.535, 656.895, 112.505, 5.3,\n",
       "   477.424, 385.851, 392.984, 0.0, 5080.0],\n",
       "  [1004.0, 2009.0, 1501.042, 1352.151, 44.628, 28.855, 746.906, 79.37, 5.8,\n",
       "   496.904, 389.641, 336.191, 0.0, 5080.0],\n",
       "  [1004.0, 2010.0, 1703.727, 1775.782, 73.139, 124.879, 835.845, 57.433,\n",
       "   6.1, 507.274, 467.485, 329.802, 2.983, 5080.0],\n",
       "  [1004.0, 2011.0, 2195.653, 2074.498, 67.723, 91.218, 864.649, 67.72, 6.7,\n",
       "   599.752, 486.582, 669.489, 12.081, 5080.0],\n",
       "  [1004.0, 2012.0, 2136.9, 2167.1, 55.0, 37.6, 918.6, 75.3, 6.3, 582.9,\n",
       "   542.4, 622.2, 12.8, 5080.0],\n",
       "  [1004.0, 2013.0, 2199.5, 2035.0, 72.9, 26.5, 999.5, 89.2, 5.8, 632.9,\n",
       "   616.7, 564.3, 11.8, 5080.0],\n",
       "  [1004.0, 2014.0, 1515.0, 1594.3, -54.5, 46.3, 845.1, 54.7, 4.85, 566.7,\n",
       "   603.9, 85.0, 12.5, 5080.0],\n",
       "  [1004.0, 2015.0, 1442.1, 1662.6, 40.5, 88.4, 865.8, 31.2, 4.7, 563.7,\n",
       "   637.2, 136.1, 10.4, 5080.0],\n",
       "  [1004.0, 2016.0, 1504.1, 1767.6, 50.2, 33.6, 914.2, 10.3, 4.6, 601.1,\n",
       "   688.0, 155.3, 10.2, 5080.0],\n",
       "  [1004.0, 2017.0, 1524.7, 1748.3, 73.7, 22.0, 936.3, 41.6, 5.0, 547.9,\n",
       "   701.2, 177.2, 10.3, 5080.0]]                                            ,\n",
       " [[1013.0, 2000.0, 3970.5, 3287.9, 868.1, 375.3, 2912.7, 1354.2, 22.452,\n",
       "   486.1, 1843.8, 16.5, 0.0, 3661.0],\n",
       "  [1013.0, 2001.0, 2499.7, 2402.8, -1287.7, 241.2, 1893.4, 421.8, 12.042,\n",
       "   253.6, 495.6, 3.0, 0.0, 3661.0],\n",
       "  [1013.0, 2002.0, 1144.2, 1047.7, -1145.0, 25.6, 732.2, 279.4, 7.6, 94.9,\n",
       "   -688.0, 10.8, 0.0, 3661.0],\n",
       "  [1013.0, 2003.0, 1296.9, 773.2, -76.7, 69.5, 627.7, 746.7, 5.7, 69.5,\n",
       "   -771.2, 400.0, 0.0, 3661.0],\n",
       "  [1013.0, 2004.0, 1428.1, 784.3, 31.3, 10.3, 659.3, 501.6, 7.5, 97.8,\n",
       "   -747.1, 400.0, 0.0, 3661.0],\n",
       "  [1013.0, 2005.0, 1535.0, 1169.2, 85.5, 30.2, 773.9, 445.4, 8.2, 140.5,\n",
       "   -648.5, 400.0, 0.0, 3661.0],\n",
       "  [1013.0, 2006.0, 1611.4, 1281.9, 94.2, 33.3, 873.5, 537.7, 8.6, 165.5,\n",
       "   -567.4, 400.0, 0.0, 3661.0],\n",
       "  [1013.0, 2007.0, 1764.8, 1322.2, 113.3, 32.5, 1007.6, 581.8, 9.05, 170.2,\n",
       "   -448.2, 200.6, 0.0, 3661.0],\n",
       "  [1013.0, 2008.0, 1921.0, 1456.4, -44.4, 42.4, 914.2, 631.5, 10.6, 162.7,\n",
       "   -505.6, 650.7, 0.0, 3661.0],\n",
       "  [1013.0, 2009.0, 1343.6, 996.7, -465.7, 32.0, 356.2, 535.5, 9.05, 131.1,\n",
       "   -979.3, 651.0, 0.0, 3661.0],\n",
       "  [1013.0, 2010.0, 1474.5, 1156.6, 77.2, 29.4, 429.6, 696.9, 9.3, 106.4,\n",
       "   -918.4, 650.8, 0.0, 3661.0]]                                            ,\n",
       " [[1021.0, 2000.0, 11.608, 25.367, -0.808, 0.139, 4.455, 0.434, 0.123,\n",
       "   4.078, -7.184, 4.875, 0.0, 3844.0],\n",
       "  [1021.0, 2001.0, 8.635, 24.051, -1.738, 0.203, 2.717, 0.198, 0.106,\n",
       "   3.488, -8.922, 3.873, 0.0, 3844.0],\n",
       "  [1021.0, 2002.0, 7.85, 20.087, 0.084, 0.034, 2.823, 0.516, 0.094, 2.816,\n",
       "   -8.816, 2.546, 0.0, 3844.0],\n",
       "  [1021.0, 2003.0, 6.044, 18.044, -0.218, 0.221, 1.32, 0.658, 0.09, 2.483,\n",
       "   -10.318, 0.631, 0.0, 3844.0],\n",
       "  [1021.0, 2004.0, 6.245, 19.833, 1.345, 0.108, 2.665, 0.332, 0.083, 2.704,\n",
       "   -8.974, 0.222, 0.0, 3844.0],\n",
       "  [1021.0, 2005.0, 8.153, 23.135, 1.9, 0.177, 4.663, 0.331, 0.084, 3.921,\n",
       "   -7.073, 0.0, 0.0, 3844.0],\n",
       "  [1021.0, 2006.0, 14.341, 24.998, 1.005, 0.205, 5.925, 5.213, 0.083,\n",
       "   4.835, -5.977, 0.0, 0.0, 3844.0],\n",
       "  [1021.0, 2007.0, 27.171, 28.72, -4.673, 0.251, 14.875, 0.922, 0.115,\n",
       "   6.395, -10.708, 5.822, 0.0, 3844.0],\n",
       "  [1021.0, 2008.0, 21.401, 34.28, -11.049, 0.202, 4.177, 0.819, 0.122,\n",
       "   6.95, -21.447, 0.023, 0.0, 3844.0]]                                     ,\n",
       " ...,\n",
       " [[324684.0, 2017.0, 51.334, 0.0, -39.892, 0.291, 35.513, 50.573, 0.047,\n",
       "   0.0, -90.284, 7.412, 0.0, 2836.0]]                                   ,\n",
       " [[325211.0, 2017.0, 361.871, 198.075, 13.905, 31.421, 280.301, 36.835,\n",
       "   0.097, 8.215, 241.062, 0.0, 6.733, 1311.0]]                         ,\n",
       " [[327451.0, 2017.0, 507.122, 409.522, -60.812, 1.212, 319.736, 46.522,\n",
       "   0.808, 9.078, -153.433, 20.79, 1.674, 4412.0]]                      ]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_old = get_ragged_constants(X_old)\n",
    "X_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dccf92a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 64)                20224     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,289\n",
      "Trainable params: 20,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#second set of hyperparameters\n",
    "max_seq = X_old.bounding_shape()[-1]\n",
    "\n",
    "mdl = tf.keras.Sequential([\n",
    "    # Input Layer with shape = [Any,  maximum sequence length]                      \n",
    "    tf.keras.layers.Input(shape=[None,max_seq], dtype=tf.float32, ragged=True),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(1, activation='softmax')\n",
    "])\n",
    "\n",
    "# CategoricalCrossentropy\n",
    "mdl.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "mdl.summary()\n",
    "#history = mdl.fit(X_old, Y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dafc351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 4)                 304       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 309\n",
      "Trainable params: 309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#second set of hyperparameters\n",
    "max_seq = X_old.bounding_shape()[-1]\n",
    "\n",
    "mdl = tf.keras.Sequential([\n",
    "    # Input Layer with shape = [Any,  maximum sequence length]                      \n",
    "    tf.keras.layers.Input(shape=[None,max_seq], dtype=tf.float32, ragged=True),\n",
    "    tf.keras.layers.LSTM(4),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# CategoricalCrossentropy\n",
    "mdl.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c61d6c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['target', 'at', 'revt', 'ib', 'capx', 'ceq', 'che', 'emp', 'invt', 're',\n",
      "       'dltt', 'dv', 'sic'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>at</th>\n",
       "      <th>revt</th>\n",
       "      <th>ib</th>\n",
       "      <th>capx</th>\n",
       "      <th>ceq</th>\n",
       "      <th>che</th>\n",
       "      <th>emp</th>\n",
       "      <th>invt</th>\n",
       "      <th>re</th>\n",
       "      <th>dltt</th>\n",
       "      <th>dv</th>\n",
       "      <th>sic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>701.854</td>\n",
       "      <td>874.255</td>\n",
       "      <td>18.531</td>\n",
       "      <td>13.134</td>\n",
       "      <td>340.212</td>\n",
       "      <td>13.809</td>\n",
       "      <td>2.500</td>\n",
       "      <td>320.590</td>\n",
       "      <td>204.065</td>\n",
       "      <td>179.987</td>\n",
       "      <td>9.157</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>710.199</td>\n",
       "      <td>638.721</td>\n",
       "      <td>-58.939</td>\n",
       "      <td>12.112</td>\n",
       "      <td>310.235</td>\n",
       "      <td>34.522</td>\n",
       "      <td>2.200</td>\n",
       "      <td>286.588</td>\n",
       "      <td>139.603</td>\n",
       "      <td>217.699</td>\n",
       "      <td>4.430</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>686.621</td>\n",
       "      <td>606.337</td>\n",
       "      <td>-12.410</td>\n",
       "      <td>9.930</td>\n",
       "      <td>294.988</td>\n",
       "      <td>29.154</td>\n",
       "      <td>2.100</td>\n",
       "      <td>259.954</td>\n",
       "      <td>124.106</td>\n",
       "      <td>164.658</td>\n",
       "      <td>0.797</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>709.292</td>\n",
       "      <td>651.958</td>\n",
       "      <td>3.504</td>\n",
       "      <td>10.286</td>\n",
       "      <td>301.684</td>\n",
       "      <td>41.010</td>\n",
       "      <td>2.300</td>\n",
       "      <td>247.245</td>\n",
       "      <td>131.884</td>\n",
       "      <td>248.666</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>732.230</td>\n",
       "      <td>747.848</td>\n",
       "      <td>18.572</td>\n",
       "      <td>13.033</td>\n",
       "      <td>314.744</td>\n",
       "      <td>50.338</td>\n",
       "      <td>2.600</td>\n",
       "      <td>255.477</td>\n",
       "      <td>142.450</td>\n",
       "      <td>227.159</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224977</th>\n",
       "      <td>0</td>\n",
       "      <td>28.426</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-40.324</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.809</td>\n",
       "      <td>21.830</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-50.268</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224978</th>\n",
       "      <td>0</td>\n",
       "      <td>29.188</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-44.810</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-8.908</td>\n",
       "      <td>22.651</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-93.370</td>\n",
       "      <td>14.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224984</th>\n",
       "      <td>0</td>\n",
       "      <td>51.334</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-39.892</td>\n",
       "      <td>0.291</td>\n",
       "      <td>35.513</td>\n",
       "      <td>50.573</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-90.284</td>\n",
       "      <td>7.412</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224985</th>\n",
       "      <td>0</td>\n",
       "      <td>361.871</td>\n",
       "      <td>198.075</td>\n",
       "      <td>13.905</td>\n",
       "      <td>31.421</td>\n",
       "      <td>280.301</td>\n",
       "      <td>36.835</td>\n",
       "      <td>0.097</td>\n",
       "      <td>8.215</td>\n",
       "      <td>241.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.733</td>\n",
       "      <td>1311.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224996</th>\n",
       "      <td>0</td>\n",
       "      <td>507.122</td>\n",
       "      <td>409.522</td>\n",
       "      <td>-60.812</td>\n",
       "      <td>1.212</td>\n",
       "      <td>319.736</td>\n",
       "      <td>46.522</td>\n",
       "      <td>0.808</td>\n",
       "      <td>9.078</td>\n",
       "      <td>-153.433</td>\n",
       "      <td>20.790</td>\n",
       "      <td>1.674</td>\n",
       "      <td>4412.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129486 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target       at     revt      ib    capx      ceq     che    emp  \\\n",
       "0            0  701.854  874.255  18.531  13.134  340.212  13.809  2.500   \n",
       "1            0  710.199  638.721 -58.939  12.112  310.235  34.522  2.200   \n",
       "2            0  686.621  606.337 -12.410   9.930  294.988  29.154  2.100   \n",
       "3            0  709.292  651.958   3.504  10.286  301.684  41.010  2.300   \n",
       "4            0  732.230  747.848  18.572  13.033  314.744  50.338  2.600   \n",
       "...        ...      ...      ...     ...     ...      ...     ...    ...   \n",
       "224977       0   28.426    0.000 -40.324   0.000    9.809  21.830  0.006   \n",
       "224978       0   29.188    0.000 -44.810   0.000   -8.908  22.651  0.008   \n",
       "224984       0   51.334    0.000 -39.892   0.291   35.513  50.573  0.047   \n",
       "224985       0  361.871  198.075  13.905  31.421  280.301  36.835  0.097   \n",
       "224996       0  507.122  409.522 -60.812   1.212  319.736  46.522  0.808   \n",
       "\n",
       "           invt       re     dltt     dv     sic  \n",
       "0       320.590  204.065  179.987  9.157  5080.0  \n",
       "1       286.588  139.603  217.699  4.430  5080.0  \n",
       "2       259.954  124.106  164.658  0.797  5080.0  \n",
       "3       247.245  131.884  248.666  0.000  5080.0  \n",
       "4       255.477  142.450  227.159  0.000  5080.0  \n",
       "...         ...      ...      ...    ...     ...  \n",
       "224977    0.000  -50.268    0.000  0.000  2836.0  \n",
       "224978    0.000  -93.370   14.057  0.000  2836.0  \n",
       "224984    0.000  -90.284    7.412  0.000  2836.0  \n",
       "224985    8.215  241.062    0.000  6.733  1311.0  \n",
       "224996    9.078 -153.433   20.790  1.674  4412.0  \n",
       "\n",
       "[129486 rows x 13 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LSTM - padding technique\n",
    "\n",
    "data4 = data.drop(['sno','ca','xrd'],1)\n",
    "data4 = data4.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "#data4 = data4.to_numpy()\n",
    "\n",
    "year_data1 = data4.drop(data3.iloc[:, 3:18], inplace = False, axis = 1)\n",
    "\n",
    "data4 = data4.drop(['gvkey','fyear'],1)\n",
    "print(data4.columns)\n",
    "\n",
    "data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a908c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "raggeddata = []\n",
    "\n",
    "start = 0\n",
    "index = 0\n",
    "end = 0\n",
    "maxLen = end - start \n",
    "while index < len(year_data1)-1: \n",
    "    cid = year_data1.iloc[index]['gvkey']\n",
    "    start = index\n",
    "    end = index\n",
    "    while end<len(year_data1):\n",
    "        if year_data1.iloc[end]['gvkey'] == year_data1.iloc[index]['gvkey']:\n",
    "            end+=1\n",
    "            \n",
    "        else:\n",
    "            #sample.append(data3[start: end])\n",
    "            raggeddata.append(data4[start: end])\n",
    "            index = end\n",
    "            break\n",
    "            \n",
    "    if maxLen < (end - start):\n",
    "        maxLen = end - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ad08f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp = {'target': 0, 'at': 0, 'revt': 0, 'ib': 0,  'capx': 0,\n",
    "       'ceq': 0, 'che': 0, 'emp': 0, 'invt': 0, 're': 0, 'dltt': 0, 'dv': 0, 'sic': 0}\n",
    "\n",
    "for i in range(0,len(raggeddata)):\n",
    "   \n",
    "    if len(raggeddata[i]) < maxLen:\n",
    "        j = len(raggeddata[i])\n",
    "        for k in range(j,maxLen):\n",
    "            raggeddata[i] = raggeddata[i].append(temp,ignore_index=True)\n",
    "            #raggeddata[i] = np.vstack([raggeddata[i],temp])\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ff81040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "  \n",
    "# create an Empty DataFrame object\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i in range(0, len(raggeddata)):\n",
    "    \n",
    "    df = df.append(raggeddata[i],ignore_index=True)\n",
    "paddedDataFrame = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c048d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.00000e+00 7.01854e+02 8.74255e+02 ... 1.79987e+02 9.15700e+00\n",
      "   5.08000e+03]\n",
      "  [0.00000e+00 7.10199e+02 6.38721e+02 ... 2.17699e+02 4.43000e+00\n",
      "   5.08000e+03]\n",
      "  [0.00000e+00 6.86621e+02 6.06337e+02 ... 1.64658e+02 7.97000e-01\n",
      "   5.08000e+03]\n",
      "  ...\n",
      "  [0.00000e+00 1.44210e+03 1.66260e+03 ... 1.36100e+02 1.04000e+01\n",
      "   5.08000e+03]\n",
      "  [0.00000e+00 1.50410e+03 1.76760e+03 ... 1.55300e+02 1.02000e+01\n",
      "   5.08000e+03]\n",
      "  [0.00000e+00 1.52470e+03 1.74830e+03 ... 1.77200e+02 1.03000e+01\n",
      "   5.08000e+03]]\n",
      "\n",
      " [[0.00000e+00 7.10199e+02 6.38721e+02 ... 2.17699e+02 4.43000e+00\n",
      "   5.08000e+03]\n",
      "  [0.00000e+00 6.86621e+02 6.06337e+02 ... 1.64658e+02 7.97000e-01\n",
      "   5.08000e+03]\n",
      "  [0.00000e+00 7.09292e+02 6.51958e+02 ... 2.48666e+02 0.00000e+00\n",
      "   5.08000e+03]\n",
      "  ...\n",
      "  [0.00000e+00 1.50410e+03 1.76760e+03 ... 1.55300e+02 1.02000e+01\n",
      "   5.08000e+03]\n",
      "  [0.00000e+00 1.52470e+03 1.74830e+03 ... 1.77200e+02 1.03000e+01\n",
      "   5.08000e+03]\n",
      "  [0.00000e+00 3.97050e+03 3.28790e+03 ... 1.65000e+01 0.00000e+00\n",
      "   3.66100e+03]]\n",
      "\n",
      " [[0.00000e+00 6.86621e+02 6.06337e+02 ... 1.64658e+02 7.97000e-01\n",
      "   5.08000e+03]\n",
      "  [0.00000e+00 7.09292e+02 6.51958e+02 ... 2.48666e+02 0.00000e+00\n",
      "   5.08000e+03]\n",
      "  [0.00000e+00 7.32230e+02 7.47848e+02 ... 2.27159e+02 0.00000e+00\n",
      "   5.08000e+03]\n",
      "  ...\n",
      "  [0.00000e+00 1.52470e+03 1.74830e+03 ... 1.77200e+02 1.03000e+01\n",
      "   5.08000e+03]\n",
      "  [0.00000e+00 3.97050e+03 3.28790e+03 ... 1.65000e+01 0.00000e+00\n",
      "   3.66100e+03]\n",
      "  [0.00000e+00 2.49970e+03 2.40280e+03 ... 3.00000e+00 0.00000e+00\n",
      "   3.66100e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "   0.00000e+00]\n",
      "  [0.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "   0.00000e+00]\n",
      "  [0.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "   0.00000e+00]\n",
      "  ...\n",
      "  [0.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "   0.00000e+00]\n",
      "  [0.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "   0.00000e+00]\n",
      "  [0.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "   0.00000e+00]]\n",
      "\n",
      " [[0.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "   0.00000e+00]\n",
      "  [0.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "   0.00000e+00]\n",
      "  [0.00000e+00 3.61871e+02 1.98075e+02 ... 0.00000e+00 6.73300e+00\n",
      "   1.31100e+03]\n",
      "  ...\n",
      "  [0.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "   0.00000e+00]\n",
      "  [0.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "   0.00000e+00]\n",
      "  [0.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "   0.00000e+00]]\n",
      "\n",
      " [[0.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "   0.00000e+00]\n",
      "  [0.00000e+00 3.61871e+02 1.98075e+02 ... 0.00000e+00 6.73300e+00\n",
      "   1.31100e+03]\n",
      "  [0.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "   0.00000e+00]\n",
      "  ...\n",
      "  [0.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "   0.00000e+00]\n",
      "  [0.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "   0.00000e+00]\n",
      "  [0.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "   0.00000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def split_data(paddedDataFrame, lookback):\n",
    "    data_raw = paddedDataFrame.to_numpy() # convert to numpy array\n",
    "    data_new = []\n",
    "    \n",
    "    # create all possible sequences of length seq_len\n",
    "    for index in range(len(data_raw) - lookback): \n",
    "        data_new.append(data_raw[index: index + lookback])\n",
    "    \n",
    "    data_new = np.array(data_new);\n",
    "    test_set_size = int(np.round(0.2*data_new.shape[0]));\n",
    "    train_set_size = data_new.shape[0] - (test_set_size);\n",
    "    \n",
    "    x_train = data_new[:train_set_size,:-1,:]\n",
    "    y_train = data_new[:train_set_size,-1,:]\n",
    "    \n",
    "    x_test = data_new[train_set_size:,:-1]\n",
    "    y_test = data_new[train_set_size:,-1,:]\n",
    "    \n",
    "    print(data_new)\n",
    "    \n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "lookback = 18 # choose sequence length\n",
    "x_train, y_train, x_test, y_test = split_data(paddedDataFrame, lookback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "899044f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.Tensor)\n",
    "y_train_lstm = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "y_test_lstm = torch.from_numpy(y_test).type(torch.Tensor)\n",
    "y_train_gru = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "y_test_gru = torch.from_numpy(y_test).type(torch.Tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b21e36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 13\n",
    "hidden_dim = 1\n",
    "num_layers = 1\n",
    "output_dim = 13\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91dfde67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6aca636",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59d9ae45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000e+00, 7.0185e+02, 8.7426e+02,  ..., 1.7999e+02,\n",
       "          9.1570e+00, 5.0800e+03],\n",
       "         [0.0000e+00, 7.1020e+02, 6.3872e+02,  ..., 2.1770e+02,\n",
       "          4.4300e+00, 5.0800e+03],\n",
       "         [0.0000e+00, 6.8662e+02, 6.0634e+02,  ..., 1.6466e+02,\n",
       "          7.9700e-01, 5.0800e+03],\n",
       "         ...,\n",
       "         [0.0000e+00, 1.5150e+03, 1.5943e+03,  ..., 8.5000e+01,\n",
       "          1.2500e+01, 5.0800e+03],\n",
       "         [0.0000e+00, 1.4421e+03, 1.6626e+03,  ..., 1.3610e+02,\n",
       "          1.0400e+01, 5.0800e+03],\n",
       "         [0.0000e+00, 1.5041e+03, 1.7676e+03,  ..., 1.5530e+02,\n",
       "          1.0200e+01, 5.0800e+03]],\n",
       "\n",
       "        [[0.0000e+00, 7.1020e+02, 6.3872e+02,  ..., 2.1770e+02,\n",
       "          4.4300e+00, 5.0800e+03],\n",
       "         [0.0000e+00, 6.8662e+02, 6.0634e+02,  ..., 1.6466e+02,\n",
       "          7.9700e-01, 5.0800e+03],\n",
       "         [0.0000e+00, 7.0929e+02, 6.5196e+02,  ..., 2.4867e+02,\n",
       "          0.0000e+00, 5.0800e+03],\n",
       "         ...,\n",
       "         [0.0000e+00, 1.4421e+03, 1.6626e+03,  ..., 1.3610e+02,\n",
       "          1.0400e+01, 5.0800e+03],\n",
       "         [0.0000e+00, 1.5041e+03, 1.7676e+03,  ..., 1.5530e+02,\n",
       "          1.0200e+01, 5.0800e+03],\n",
       "         [0.0000e+00, 1.5247e+03, 1.7483e+03,  ..., 1.7720e+02,\n",
       "          1.0300e+01, 5.0800e+03]],\n",
       "\n",
       "        [[0.0000e+00, 6.8662e+02, 6.0634e+02,  ..., 1.6466e+02,\n",
       "          7.9700e-01, 5.0800e+03],\n",
       "         [0.0000e+00, 7.0929e+02, 6.5196e+02,  ..., 2.4867e+02,\n",
       "          0.0000e+00, 5.0800e+03],\n",
       "         [0.0000e+00, 7.3223e+02, 7.4785e+02,  ..., 2.2716e+02,\n",
       "          0.0000e+00, 5.0800e+03],\n",
       "         ...,\n",
       "         [0.0000e+00, 1.5041e+03, 1.7676e+03,  ..., 1.5530e+02,\n",
       "          1.0200e+01, 5.0800e+03],\n",
       "         [0.0000e+00, 1.5247e+03, 1.7483e+03,  ..., 1.7720e+02,\n",
       "          1.0300e+01, 5.0800e+03],\n",
       "         [0.0000e+00, 3.9705e+03, 3.2879e+03,  ..., 1.6500e+01,\n",
       "          0.0000e+00, 3.6610e+03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c9c408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 MSE:  331069408.0\n",
      "Epoch  1 MSE:  331069376.0\n",
      "Epoch  2 MSE:  331069376.0\n",
      "Epoch  3 MSE:  331069376.0\n",
      "Epoch  4 MSE:  331069344.0\n",
      "Epoch  5 MSE:  331069312.0\n",
      "Epoch  6 MSE:  331069248.0\n",
      "Epoch  7 MSE:  331069248.0\n",
      "Epoch  8 MSE:  331069248.0\n",
      "Epoch  9 MSE:  331069216.0\n",
      "Epoch  10 MSE:  331069216.0\n",
      "Epoch  11 MSE:  331069152.0\n",
      "Epoch  12 MSE:  331069152.0\n",
      "Epoch  13 MSE:  331069120.0\n",
      "Epoch  14 MSE:  331069120.0\n",
      "Epoch  15 MSE:  331069120.0\n",
      "Epoch  16 MSE:  331069056.0\n",
      "Epoch  17 MSE:  331069056.0\n",
      "Epoch  18 MSE:  331069024.0\n",
      "Epoch  19 MSE:  331068992.0\n",
      "Epoch  20 MSE:  331068992.0\n",
      "Epoch  21 MSE:  331068928.0\n",
      "Epoch  22 MSE:  331068864.0\n",
      "Epoch  23 MSE:  331068864.0\n",
      "Epoch  24 MSE:  331068832.0\n",
      "Epoch  25 MSE:  331068832.0\n",
      "Epoch  26 MSE:  331068800.0\n",
      "Epoch  27 MSE:  331068768.0\n",
      "Epoch  28 MSE:  331068768.0\n",
      "Epoch  29 MSE:  331068736.0\n",
      "Epoch  30 MSE:  331068672.0\n",
      "Epoch  31 MSE:  331068640.0\n",
      "Epoch  32 MSE:  331068640.0\n",
      "Epoch  33 MSE:  331068576.0\n",
      "Epoch  34 MSE:  331068544.0\n",
      "Epoch  35 MSE:  331068512.0\n",
      "Epoch  36 MSE:  331068480.0\n",
      "Epoch  37 MSE:  331068448.0\n",
      "Epoch  38 MSE:  331068384.0\n",
      "Epoch  39 MSE:  331068384.0\n",
      "Epoch  40 MSE:  331068320.0\n",
      "Epoch  41 MSE:  331068288.0\n",
      "Epoch  42 MSE:  331068288.0\n",
      "Epoch  43 MSE:  331068256.0\n",
      "Epoch  44 MSE:  331068224.0\n",
      "Epoch  45 MSE:  331068192.0\n",
      "Epoch  46 MSE:  331068128.0\n",
      "Epoch  47 MSE:  331068096.0\n",
      "Epoch  48 MSE:  331068064.0\n",
      "Epoch  49 MSE:  331068064.0\n",
      "Epoch  50 MSE:  331068032.0\n",
      "Epoch  51 MSE:  331068032.0\n",
      "Epoch  52 MSE:  331067968.0\n",
      "Epoch  53 MSE:  331067968.0\n",
      "Epoch  54 MSE:  331067936.0\n",
      "Epoch  55 MSE:  331067872.0\n",
      "Epoch  56 MSE:  331067840.0\n",
      "Epoch  57 MSE:  331067776.0\n",
      "Epoch  58 MSE:  331067776.0\n",
      "Epoch  59 MSE:  331067776.0\n",
      "Epoch  60 MSE:  331067680.0\n",
      "Epoch  61 MSE:  331067680.0\n",
      "Epoch  62 MSE:  331067680.0\n",
      "Epoch  63 MSE:  331067616.0\n",
      "Epoch  64 MSE:  331067616.0\n",
      "Epoch  65 MSE:  331067584.0\n",
      "Epoch  66 MSE:  331067552.0\n",
      "Epoch  67 MSE:  331067488.0\n",
      "Epoch  68 MSE:  331067424.0\n",
      "Epoch  69 MSE:  331067392.0\n",
      "Epoch  70 MSE:  331067392.0\n",
      "Epoch  71 MSE:  331067360.0\n",
      "Epoch  72 MSE:  331067296.0\n",
      "Epoch  73 MSE:  331067296.0\n",
      "Epoch  74 MSE:  331067264.0\n",
      "Epoch  75 MSE:  331067232.0\n",
      "Epoch  76 MSE:  331067200.0\n",
      "Epoch  77 MSE:  331067136.0\n",
      "Epoch  78 MSE:  331067104.0\n",
      "Epoch  79 MSE:  331067040.0\n",
      "Epoch  80 MSE:  331067008.0\n",
      "Epoch  81 MSE:  331066976.0\n",
      "Epoch  82 MSE:  331066944.0\n",
      "Epoch  83 MSE:  331066912.0\n",
      "Epoch  84 MSE:  331066880.0\n",
      "Epoch  85 MSE:  331066880.0\n",
      "Epoch  86 MSE:  331066816.0\n",
      "Epoch  87 MSE:  331066688.0\n",
      "Epoch  88 MSE:  331066656.0\n",
      "Epoch  89 MSE:  331066592.0\n",
      "Epoch  90 MSE:  331066592.0\n",
      "Epoch  91 MSE:  331066528.0\n",
      "Epoch  92 MSE:  331066496.0\n",
      "Epoch  93 MSE:  331066432.0\n",
      "Epoch  94 MSE:  331066432.0\n",
      "Epoch  95 MSE:  331066336.0\n",
      "Epoch  96 MSE:  331066272.0\n",
      "Epoch  97 MSE:  331066208.0\n",
      "Epoch  98 MSE:  331066144.0\n",
      "Epoch  99 MSE:  331066144.0\n",
      "Training time: 64.50747108459473\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "hist = np.zeros(num_epochs)\n",
    "start_time = time.time()\n",
    "lstm = []\n",
    "for t in range(num_epochs):\n",
    "    y_train_pred = model(x_train)\n",
    "    loss = criterion(y_train_pred, y_train_lstm)\n",
    "    print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "    hist[t] = loss.item()\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "training_time = time.time()-start_time\n",
    "print(\"Training time: {}\".format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37eb94b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn) = self.gru(x, (h0.detach()))\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "867a13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = GRU(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
    "criterion1 = torch.nn.MSELoss(reduction='mean')\n",
    "optimiser1 = torch.optim.Adam(model1.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91f1431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 MSE:  331070016.0\n",
      "Epoch  1 MSE:  331069984.0\n",
      "Epoch  2 MSE:  331069952.0\n",
      "Epoch  3 MSE:  331069920.0\n",
      "Epoch  4 MSE:  331069920.0\n",
      "Epoch  5 MSE:  331069888.0\n",
      "Epoch  6 MSE:  331069856.0\n",
      "Epoch  7 MSE:  331069856.0\n",
      "Epoch  8 MSE:  331069824.0\n",
      "Epoch  9 MSE:  331069760.0\n",
      "Epoch  10 MSE:  331069728.0\n",
      "Epoch  11 MSE:  331069728.0\n",
      "Epoch  12 MSE:  331069728.0\n",
      "Epoch  13 MSE:  331069664.0\n",
      "Epoch  14 MSE:  331069632.0\n",
      "Epoch  15 MSE:  331069632.0\n",
      "Epoch  16 MSE:  331069600.0\n",
      "Epoch  17 MSE:  331069600.0\n",
      "Epoch  18 MSE:  331069536.0\n",
      "Epoch  19 MSE:  331069504.0\n",
      "Epoch  20 MSE:  331069504.0\n",
      "Epoch  21 MSE:  331069472.0\n",
      "Epoch  22 MSE:  331069408.0\n",
      "Epoch  23 MSE:  331069408.0\n",
      "Epoch  24 MSE:  331069376.0\n",
      "Epoch  25 MSE:  331069376.0\n",
      "Epoch  26 MSE:  331069344.0\n",
      "Epoch  27 MSE:  331069312.0\n",
      "Epoch  28 MSE:  331069312.0\n",
      "Epoch  29 MSE:  331069248.0\n",
      "Epoch  30 MSE:  331069248.0\n",
      "Epoch  31 MSE:  331069248.0\n",
      "Epoch  32 MSE:  331069184.0\n",
      "Epoch  33 MSE:  331069184.0\n",
      "Epoch  34 MSE:  331069120.0\n",
      "Epoch  35 MSE:  331069120.0\n",
      "Epoch  36 MSE:  331069056.0\n",
      "Epoch  37 MSE:  331069056.0\n",
      "Epoch  38 MSE:  331068992.0\n",
      "Epoch  39 MSE:  331068960.0\n",
      "Epoch  40 MSE:  331068960.0\n",
      "Epoch  41 MSE:  331068960.0\n",
      "Epoch  42 MSE:  331068960.0\n",
      "Epoch  43 MSE:  331068928.0\n",
      "Epoch  44 MSE:  331068864.0\n",
      "Epoch  45 MSE:  331068832.0\n",
      "Epoch  46 MSE:  331068832.0\n",
      "Epoch  47 MSE:  331068768.0\n",
      "Epoch  48 MSE:  331068768.0\n",
      "Epoch  49 MSE:  331068704.0\n",
      "Epoch  50 MSE:  331068672.0\n",
      "Epoch  51 MSE:  331068672.0\n",
      "Epoch  52 MSE:  331068640.0\n",
      "Epoch  53 MSE:  331068640.0\n",
      "Epoch  54 MSE:  331068608.0\n",
      "Epoch  55 MSE:  331068576.0\n",
      "Epoch  56 MSE:  331068576.0\n",
      "Epoch  57 MSE:  331068544.0\n",
      "Epoch  58 MSE:  331068512.0\n",
      "Epoch  59 MSE:  331068480.0\n",
      "Epoch  60 MSE:  331068416.0\n",
      "Epoch  61 MSE:  331068384.0\n",
      "Epoch  62 MSE:  331068384.0\n",
      "Epoch  63 MSE:  331068320.0\n",
      "Epoch  64 MSE:  331068320.0\n",
      "Epoch  65 MSE:  331068288.0\n",
      "Epoch  66 MSE:  331068288.0\n",
      "Epoch  67 MSE:  331068256.0\n",
      "Epoch  68 MSE:  331068256.0\n",
      "Epoch  69 MSE:  331068224.0\n",
      "Epoch  70 MSE:  331068160.0\n",
      "Epoch  71 MSE:  331068128.0\n",
      "Epoch  72 MSE:  331068096.0\n",
      "Epoch  73 MSE:  331068064.0\n",
      "Epoch  74 MSE:  331068032.0\n",
      "Epoch  75 MSE:  331068032.0\n",
      "Epoch  76 MSE:  331068000.0\n",
      "Epoch  77 MSE:  331067968.0\n",
      "Epoch  78 MSE:  331067968.0\n",
      "Epoch  79 MSE:  331067904.0\n",
      "Epoch  80 MSE:  331067904.0\n",
      "Epoch  81 MSE:  331067840.0\n",
      "Epoch  82 MSE:  331067776.0\n",
      "Epoch  83 MSE:  331067776.0\n",
      "Epoch  84 MSE:  331067744.0\n",
      "Epoch  85 MSE:  331067744.0\n",
      "Epoch  86 MSE:  331067712.0\n",
      "Epoch  87 MSE:  331067648.0\n",
      "Epoch  88 MSE:  331067616.0\n",
      "Epoch  89 MSE:  331067584.0\n",
      "Epoch  90 MSE:  331067552.0\n",
      "Epoch  91 MSE:  331067552.0\n",
      "Epoch  92 MSE:  331067488.0\n",
      "Epoch  93 MSE:  331067488.0\n",
      "Epoch  94 MSE:  331067424.0\n",
      "Epoch  95 MSE:  331067424.0\n",
      "Epoch  96 MSE:  331067360.0\n",
      "Epoch  97 MSE:  331067360.0\n",
      "Epoch  98 MSE:  331067328.0\n",
      "Epoch  99 MSE:  331067296.0\n",
      "Training time: 55.816899061203\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "hist = np.zeros(num_epochs)\n",
    "start_time = time.time()\n",
    "gru = []\n",
    "for t in range(num_epochs):\n",
    "    y_train_pred = model1(x_train)\n",
    "    loss = criterion1(y_train_pred, y_train_gru)\n",
    "    print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "    hist[t] = loss.item()\n",
    "    optimiser1.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser1.step()\n",
    "    \n",
    "training_time = time.time()-start_time\n",
    "print(\"Training time: {}\".format(training_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
